{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/redditsentiment.csv'\n",
    "reddit_data = pd.read_csv(input_file)\n",
    "reddit_data['date_only'] = pd.to_datetime(reddit_data['date_only'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stock_data(tickers, start_date, end_date):\n",
    "    stock_data=[]\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data=yf.download(ticker, start_date, end=end_date)\n",
    "            if not data.empty:\n",
    "                data['ticker']=ticker\n",
    "                \n",
    "                data['date_only'] = data.index.date\n",
    "\n",
    "                data.columns = [f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col for col in data.columns]\n",
    "\n",
    "                data.rename(columns={col: 'date_only' if 'date_only' in col else col for col in data.columns}, inplace=True)\n",
    "                print(f\"Date_only column added for {ticker}.columns: {data.columns}\")\n",
    "                stock_data.append(data)\n",
    "            else:\n",
    "                print(f\"No data for {ticker}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error for {ticker}: {e}\")\n",
    "    \n",
    "    if stock_data:\n",
    "        combined_data = pd.concat(stock_data, ignore_index=True)\n",
    "        print(f\"Combined stock data columns: {combined_data.columns}\")\n",
    "    else:\n",
    "        combined_data = pd.DataFrame()  \n",
    "        print(\"No stock data fetched.\")\n",
    "\n",
    "    \n",
    "    if 'date_only' in combined_data.columns:\n",
    "        combined_data['date_only'] = pd.to_datetime(combined_data['date_only'])\n",
    "    else:\n",
    "        print(\"Warning: 'date_only' column not found in combined stock data.\")\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_only column added for TSLA.columns: Index(['Close_TSLA', 'High_TSLA', 'Low_TSLA', 'Open_TSLA', 'Volume_TSLA',\n",
      "       'ticker_', 'date_only'],\n",
      "      dtype='object')\n",
      "Date_only column added for GME.columns: Index(['Close_GME', 'High_GME', 'Low_GME', 'Open_GME', 'Volume_GME', 'ticker_',\n",
      "       'date_only'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_only column added for AAPL.columns: Index(['Close_AAPL', 'High_AAPL', 'Low_AAPL', 'Open_AAPL', 'Volume_AAPL',\n",
      "       'ticker_', 'date_only'],\n",
      "      dtype='object')\n",
      "Date_only column added for NVDA.columns: Index(['Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA',\n",
      "       'ticker_', 'date_only'],\n",
      "      dtype='object')\n",
      "Combined stock data columns: Index(['Close_TSLA', 'High_TSLA', 'Low_TSLA', 'Open_TSLA', 'Volume_TSLA',\n",
      "       'ticker_', 'date_only', 'Close_GME', 'High_GME', 'Low_GME', 'Open_GME',\n",
      "       'Volume_GME', 'Close_AAPL', 'High_AAPL', 'Low_AAPL', 'Open_AAPL',\n",
      "       'Volume_AAPL', 'Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA',\n",
      "       'Volume_NVDA'],\n",
      "      dtype='object')\n",
      "date_only column exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tickers = ['TSLA', 'GME', 'AAPL', 'NVDA']\n",
    "start_date = (datetime.now() - pd.Timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "combined_reddit_stock = scrape_stock_data(tickers, start_date, end_date)\n",
    "if 'date_only' in combined_reddit_stock.columns:\n",
    "   print('date_only column exists')\n",
    "else:\n",
    "   print('date_only column does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Close_TSLA   High_TSLA    Low_TSLA   Open_TSLA  Volume_TSLA ticker_  \\\n",
      "0  147.050003  150.940002  146.220001  148.970001   86005100.0    TSLA   \n",
      "1  142.050003  144.440002  138.800003  140.559998  107097600.0    TSLA   \n",
      "2  144.679993  147.259995  141.110001  143.330002  124545100.0    TSLA   \n",
      "3  162.130005  167.970001  157.509995  162.839996  181178000.0    TSLA   \n",
      "4  170.179993  170.880005  158.360001  158.960007  126427500.0    TSLA   \n",
      "\n",
      "   date_only  Close_GME  High_GME  Low_GME  ...  Close_AAPL  High_AAPL  \\\n",
      "0 2024-04-19        NaN       NaN      NaN  ...         NaN        NaN   \n",
      "1 2024-04-22        NaN       NaN      NaN  ...         NaN        NaN   \n",
      "2 2024-04-23        NaN       NaN      NaN  ...         NaN        NaN   \n",
      "3 2024-04-24        NaN       NaN      NaN  ...         NaN        NaN   \n",
      "4 2024-04-25        NaN       NaN      NaN  ...         NaN        NaN   \n",
      "\n",
      "   Low_AAPL  Open_AAPL  Volume_AAPL  Close_NVDA  High_NVDA  Low_NVDA  \\\n",
      "0       NaN        NaN          NaN         NaN        NaN       NaN   \n",
      "1       NaN        NaN          NaN         NaN        NaN       NaN   \n",
      "2       NaN        NaN          NaN         NaN        NaN       NaN   \n",
      "3       NaN        NaN          NaN         NaN        NaN       NaN   \n",
      "4       NaN        NaN          NaN         NaN        NaN       NaN   \n",
      "\n",
      "   Open_NVDA  Volume_NVDA  \n",
      "0        NaN          NaN  \n",
      "1        NaN          NaN  \n",
      "2        NaN          NaN  \n",
      "3        NaN          NaN  \n",
      "4        NaN          NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_reddit_stock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2024-04-19\n",
      "1   2024-04-22\n",
      "2   2024-04-23\n",
      "3   2024-04-24\n",
      "4   2024-04-25\n",
      "Name: date_only, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "combined_reddit_stock['date_only'] = pd.to_datetime(combined_reddit_stock['date_only'], errors='coerce')\n",
    "reddit_data['date_only'] = pd.to_datetime(reddit_data['date_only'], errors='coerce')\n",
    "print(combined_reddit_stock['date_only'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subreddit              created  score  upvote_ratio  num_comments  \\\n",
      "0  wallstreetbets  2025-04-19 09:32:11     41          0.92            18   \n",
      "1  wallstreetbets  2025-04-19 07:40:37    106          0.94            70   \n",
      "2  wallstreetbets  2025-04-19 03:58:09     87          0.81            84   \n",
      "3  wallstreetbets  2025-04-18 20:35:36    163          0.91            78   \n",
      "4  wallstreetbets  2025-04-18 19:42:14   7788          0.91          1623   \n",
      "\n",
      "   has_target_stock mentioned_tickers  date_only  title_sentiment_vader  \\\n",
      "0                 0               NaN 2025-04-19                 0.2732   \n",
      "1                 0               NaN 2025-04-19                 0.3400   \n",
      "2                 0               NaN 2025-04-19                 0.4215   \n",
      "3                 0               NaN 2025-04-18                 0.3612   \n",
      "4                 0               NaN 2025-04-18                -0.4215   \n",
      "\n",
      "   post_sentiment_vader  title_sentiment_finbert  post_sentiment_finbert  \\\n",
      "0                0.0000                 0.940416                0.000000   \n",
      "1                0.9393                 0.000000                0.000000   \n",
      "2                0.0000                 0.000000                0.000000   \n",
      "3               -0.1691                 0.000000               -0.606552   \n",
      "4                0.0000                -0.812795                0.000000   \n",
      "\n",
      "   title_weight  post_weight  vader_score  finbert_score  sentiment_score  \\\n",
      "0          1.00         0.00     0.273200       0.940416         0.606808   \n",
      "1          0.75         0.25     0.489825       0.000000         0.244913   \n",
      "2          0.75         0.25     0.316125       0.000000         0.158062   \n",
      "3          0.75         0.25     0.228625      -0.151638         0.038494   \n",
      "4          0.75         0.25    -0.316125      -0.609596        -0.462861   \n",
      "\n",
      "   sentiment_normalized sentiment_category  \n",
      "0             80.340404      very_positive  \n",
      "1             62.245625           positive  \n",
      "2             57.903125           positive  \n",
      "3             51.924677            neutral  \n",
      "4             26.856963           negative  \n"
     ]
    }
   ],
   "source": [
    "print(reddit_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Close_TSLA   High_TSLA    Low_TSLA   Open_TSLA  Volume_TSLA ticker_  \\\n",
      "0  273.130005  291.850006  271.820007  272.480011  162572100.0    TSLA   \n",
      "1  273.130005  291.850006  271.820007  272.480011  162572100.0    TSLA   \n",
      "2  263.549988  276.100006  260.570007  275.579987  123809400.0    TSLA   \n",
      "3  263.549988  276.100006  260.570007  275.579987  123809400.0    TSLA   \n",
      "4  263.549988  276.100006  260.570007  275.579987  123809400.0    TSLA   \n",
      "\n",
      "   date_only  Close_GME  High_GME  Low_GME  ...  post_sentiment_vader  \\\n",
      "0 2025-03-27        NaN       NaN      NaN  ...                0.9474   \n",
      "1 2025-03-27        NaN       NaN      NaN  ...                0.7845   \n",
      "2 2025-03-28        NaN       NaN      NaN  ...               -0.8611   \n",
      "3 2025-03-28        NaN       NaN      NaN  ...                0.7184   \n",
      "4 2025-03-28        NaN       NaN      NaN  ...                0.9638   \n",
      "\n",
      "   title_sentiment_finbert  post_sentiment_finbert  title_weight  post_weight  \\\n",
      "0                  0.00000                     0.0          0.75         0.25   \n",
      "1                  0.00000                     0.0          0.75         0.25   \n",
      "2                 -0.88191                     0.0          0.75         0.25   \n",
      "3                  0.00000                     0.0          0.75         0.25   \n",
      "4                  0.00000                     0.0          0.75         0.25   \n",
      "\n",
      "   vader_score  finbert_score  sentiment_score  sentiment_normalized  \\\n",
      "0     0.236850       0.000000         0.118425             55.921250   \n",
      "1     0.196125       0.000000         0.098062             54.903125   \n",
      "2     0.155150      -0.661432        -0.253141             37.342939   \n",
      "3     0.586325       0.000000         0.293162             64.658125   \n",
      "4     0.502325       0.000000         0.251162             62.558125   \n",
      "\n",
      "   sentiment_category  \n",
      "0            positive  \n",
      "1             neutral  \n",
      "2            negative  \n",
      "3            positive  \n",
      "4            positive  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "combined_reddit_stock = combined_reddit_stock.merge(reddit_data, on='date_only', how='inner')\n",
    "print(combined_reddit_stock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file created\n"
     ]
    }
   ],
   "source": [
    "combined_reddit_stock.drop(columns=['date_only', 'created'], inplace=True)\n",
    "output_file = '../data/stock_reddit_merge.csv'\n",
    "combined_reddit_stock.to_csv(output_file, index=False)\n",
    "print('output file created')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
