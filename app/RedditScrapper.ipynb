{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime, timedelta\n",
    "from getpass import getpass\n",
    "import praw\n",
    "import prawcore\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "# Enable MPS fallback for operations not supported on the GPU\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Select device: use MPS (Metal Performance Shaders) if available, otherwise CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely input credentials (enter your credentials when prompted)\n",
    "client_id = getpass(\"Enter your Reddit Client ID: \")\n",
    "client_secret = getpass(\"Enter your Reddit Client Secret (or press Enter for None): \")\n",
    "client_id     = getpass(\"Enter your Reddit Client ID: \")\n",
    "client_secret = getpass(\"Enter your Reddit Client Secret: \")\n",
    "username      = getpass(\"Enter your Reddit username: \")\n",
    "password      = getpass(\"Enter your Reddit password: \")\n",
    "if client_secret.strip() == \"\":\n",
    "    client_secret = None  # Use None for installed apps\n",
    "\n",
    "user_agent = \"Stock prediction by /u/Stoic_Gaze\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing user mentions, emojis, and common stopwords\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):  # Check if text is a string\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove user mentions (e.g. u/username)\n",
    "    text = re.sub(r'/?u/\\w+', '', text)\n",
    "    \n",
    "    # Remove emojis - simple approach to remove most common emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def check_stock_mentions(text, target_stocks):\n",
    "    \"\"\"\n",
    "    Check if text contains mentions of target stocks\n",
    "    Returns: (has_mention, mentioned_tickers)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):  # Check if text is a string\n",
    "        return 0, []\n",
    "    \n",
    "    # Define patterns for stock mentions\n",
    "    # Match both $TICKER and just TICKER\n",
    "    mentioned_tickers = []\n",
    "    \n",
    "    for ticker in target_stocks:\n",
    "        # Remove $ if present for comparison\n",
    "        clean_ticker = ticker.replace('$', '')\n",
    "        \n",
    "        # Check for ticker with or without $ prefix\n",
    "        pattern = r'(\\$' + clean_ticker + r'\\b|\\b' + clean_ticker + r'\\b)'\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            mentioned_tickers.append(clean_ticker)\n",
    "    \n",
    "    has_mention = 1 if mentioned_tickers else 0\n",
    "    return has_mention, mentioned_tickers\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove common stopwords from text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):  # Check if text is a string\n",
    "        return \"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redditScrape(reddit, subredditName, start_date, end_date, target_stocks=['TSLA', 'GME', 'AAPL', 'NVDA'], post_limit=1000):\n",
    "    \"\"\"\n",
    "    Scrape Reddit posts with stock-specific enhancements\n",
    "    \n",
    "    Parameters:\n",
    "    - reddit: Reddit API instance\n",
    "    - subredditName: Subreddit to scrape\n",
    "    - start_date: Datetime object for starting date\n",
    "    - end_date: Datetime object for ending date\n",
    "    - target_stocks: List of stock tickers to specifically track\n",
    "    - post_limit: Maximum number of posts to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with Reddit posts and stock-specific data\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subredditName)\n",
    "    posts = []\n",
    "    \n",
    "    # Normalize target_stocks to ensure consistent processing\n",
    "    target_stocks = [ticker.replace('$', '').upper() for ticker in target_stocks]\n",
    "    \n",
    "    try:\n",
    "        for post in subreddit.new(limit=post_limit):\n",
    "            post_date = datetime.fromtimestamp(post.created)\n",
    "            \n",
    "            if start_date <= post_date <= end_date:\n",
    "                # Clean title and selftext for processing\n",
    "                clean_title = clean_text(post.title)\n",
    "                clean_selftext = clean_text(post.selftext)\n",
    "                \n",
    "                # Combined text for checking stock mentions\n",
    "                combined_text = f\"{clean_title} {clean_selftext}\"\n",
    "                \n",
    "                # Check for stock mentions\n",
    "                has_mention, mentioned_tickers = check_stock_mentions(combined_text, target_stocks)\n",
    "                \n",
    "                # Process text to remove stopwords\n",
    "                processed_title = remove_stopwords(clean_title)\n",
    "                processed_selftext = remove_stopwords(clean_selftext)\n",
    "                \n",
    "                posts.append({\n",
    "                    'subreddit': subredditName,\n",
    "                    'title': post.title,\n",
    "                    'clean_title': processed_title,\n",
    "                    'id': post.id,\n",
    "                    'author': str(post.author),\n",
    "                    'created': post_date,\n",
    "                    'score': post.score,\n",
    "                    'upvote_ratio': post.upvote_ratio,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'url': post.url,\n",
    "                    'selftext': post.selftext,\n",
    "                    'clean_selftext': processed_selftext,\n",
    "                    'has_target_stock': has_mention,\n",
    "                    'mentioned_tickers': ','.join(mentioned_tickers) if mentioned_tickers else ''\n",
    "                })\n",
    "                \n",
    "    except prawcore.exceptions.ResponseException as e:\n",
    "        print(f\"Reddit API returned an error: {e}. Waiting 60 seconds before retrying...\")\n",
    "        time.sleep(60)\n",
    "        return redditScrape(reddit, subredditName, start_date, end_date, target_stocks, post_limit)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame(posts) if posts else pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/wallstreetbets...\n",
      "Reddit API returned an error: received 401 HTTP response. Waiting 60 seconds before retrying...\n",
      "Reddit API returned an error: received 401 HTTP response. Waiting 60 seconds before retrying...\n",
      "Reddit API returned an error: received 401 HTTP response. Waiting 60 seconds before retrying...\n"
     ]
    }
   ],
   "source": [
    "# Define date range for scraping\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365)  # Last 365 days\n",
    "\n",
    "# Target stocks to track \\\n",
    "target_stocks = ['TSLA', '$GME', 'AAPL', '$NVDA']\n",
    "\n",
    "# Subreddits to scrape\n",
    "subreddits = ['wallstreetbets', 'stocks', 'investing']\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Scraping r/{subreddit}...\")\n",
    "    df = redditScrape(reddit, subreddit, start_date, end_date, target_stocks)\n",
    "    \n",
    "    if not df.empty:\n",
    "        df['subreddit'] = subreddit\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        \n",
    "    # Rate Limiting for API\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/jonathanermias/Documents/GitHub/RedditStocks_v2.0/data/reddit_stock_scrape\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mall_data\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping complete, Data saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print summary statistics\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "cols_to_drop = ['id', 'url', 'author']\n",
    "# Drop columns that are not needed\n",
    "# Check if columns exist before dropping\n",
    "all_data = all_data.drop(columns=[c for c in cols_to_drop if c in all_data.columns])\n",
    "\n",
    "print(\"Columns after drop:\", all_data.columns.tolist())\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"/Users/jonathanermias/Documents/GitHub/RedditStocks_v2.0/data/reddit_stock_scrape{timestamp}.csv\"\n",
    "all_data.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Scraping complete, Data saved to {filename}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nSCRAPE SUMMARY:\")\n",
    "print(f\"Total posts scraped: {len(all_data)}\")\n",
    "stock_mentions = all_data[all_data['has_target_stock'] == 1]\n",
    "print(f\"Posts with target stock mentions: {len(stock_mentions)}\")\n",
    "\n",
    "# Count by ticker (posts may mention multiple tickers)\n",
    "ticker_counts = {}\n",
    "for tickers in stock_mentions['mentioned_tickers'].dropna():\n",
    "    if tickers:\n",
    "        for ticker in tickers.split(','):\n",
    "            ticker_counts[ticker] = ticker_counts.get(ticker, 0) + 1\n",
    "\n",
    "print(\"\\nMentions by ticker:\")\n",
    "for ticker, count in ticker_counts.items():\n",
    "    print(f\"- {ticker}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preview the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m display(\u001b[43mall_data\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Basic statistics\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBasic Statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Preview the data\n",
    "display(all_data.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Most upvoted post: {all_data['score'].max()} upvotes\")\n",
    "print(f\"Average upvote ratio: {all_data['upvote_ratio'].mean():.2f}\")\n",
    "print(f\"Average number of comments: {all_data['num_comments'].mean():.1f}\")\n",
    "\n",
    "# Distribution of posts by subreddit\n",
    "subreddit_counts = all_data['subreddit'].value_counts()\n",
    "display(subreddit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Filter posts with stock mentions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stock_posts \u001b[38;5;241m=\u001b[39m \u001b[43mall_data\u001b[49m[all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_target_stock\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate average engagement for stock-related posts\u001b[39;00m\n\u001b[1;32m      5\u001b[0m stock_avg_score \u001b[38;5;241m=\u001b[39m stock_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter posts with stock mentions\n",
    "stock_posts = all_data[all_data['has_target_stock'] == 1].copy()\n",
    "\n",
    "# Calculate average engagement for stock-related posts\n",
    "stock_avg_score = stock_posts['score'].mean()\n",
    "stock_avg_comments = stock_posts['num_comments'].mean()\n",
    "\n",
    "# Calculate average engagement for non-stock posts\n",
    "non_stock_posts = all_data[all_data['has_target_stock'] == 0]\n",
    "non_stock_avg_score = non_stock_posts['score'].mean()\n",
    "non_stock_avg_comments = non_stock_posts['num_comments'].mean()\n",
    "\n",
    "print(\"\\nEngagement Comparison:\")\n",
    "print(f\"Stock-related posts average score: {stock_avg_score:.1f}\")\n",
    "print(f\"Non-stock posts average score: {non_stock_avg_score:.1f}\")\n",
    "print(f\"Stock-related posts average comments: {stock_avg_comments:.1f}\")\n",
    "print(f\"Non-stock posts average comments: {non_stock_avg_comments:.1f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# Create a bar chart of ticker mentions\n",
    "plt.figure(figsize=(10,6))\n",
    "tickers = list(ticker_counts.keys())\n",
    "counts = list(ticker_counts.values())\n",
    "plt.bar(tickers, counts, color='skyblue')\n",
    "plt.title('Stock Ticker Mentions')\n",
    "plt.xlabel('Ticker')\n",
    "plt.ylabel('Number of Mentions')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RedditStocksV2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
